{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple consumption-saving problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Imports](#toc1_)    \n",
    "- 2. [Model](#toc2_)    \n",
    "- 3. [Defining the model class](#toc3_)    \n",
    "  - 3.1. [Setup and allocate](#toc3_1_)    \n",
    "  - 3.2. [Drawing shocks and initial states](#toc3_2_)    \n",
    "  - 3.3. [Quadrature](#toc3_3_)    \n",
    "  - 3.4. [Outcomes](#toc3_4_)    \n",
    "  - 3.5. [Reward](#toc3_5_)    \n",
    "  - 3.6. [Transition functions](#toc3_6_)    \n",
    "  - 3.7. [Terminal actions](#toc3_7_)    \n",
    "- 4. [DeepSimulate](#toc4_)    \n",
    "- 5. [DeepVPD](#toc5_)    \n",
    "- 6. [DeepFOC](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE' # without this Python may crash when plotting from matplotlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consav.quadrature import log_normal_gauss_hermite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EconDLSolvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to solve a **simple consumption-saving model**.\n",
    "\n",
    "1. A single state: cash-on-hand $m_t$\n",
    "2. A single transitory shock $\\psi_t \\sim \\log \\mathcal{N}\\left(-\\frac{1}{2}\\sigma_{\\psi}^2,\\sigma_{\\psi}^2\\right) \\\\$ \n",
    "3. A single choice: consumption $c_t$, or equivalently the savings rate, $a_t\\in[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function is\n",
    "\n",
    "$$\n",
    "u(c_t) = \\log (c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bellman equation is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t(m_t) &= \\underset{a_t\\in[0,1]}{\\max} \\ u(c_t) + \\beta E_t[v_{t+1}(m_{t+1})] \\\\\n",
    "&\\text{s.t} \\\\\n",
    "c_t &= a_t m_t\\\\ \n",
    "\\bar{m}_t &= m_t - c_t \\\\ \n",
    "m_{t+1} &= (1+r) \\bar{m}_t + \\kappa \\cdot \\psi_{t+1} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve with three different methods:**\n",
    "\n",
    "1. DeepSimulate\n",
    "2. DeepVPD\n",
    "3. DeepFOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Defining the model class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the model class based on the **DLSolverclass** from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(EconDLSolvers.DLSolverClass): # inherit from DLSolverClass\n",
    "\n",
    "    # setup and allocate\n",
    "    def setup(self): pass\n",
    "    def allocate(self): pass\n",
    "    def setup_train(self): pass\n",
    "    def allocate_train(self): pass\n",
    "\n",
    "    # draw\n",
    "    def draw_initial_states(self): pass\n",
    "    def draw_shocks(self): pass\n",
    "\n",
    "    # transition\n",
    "    def outcomes(self): pass # states, actions -> (intermediary) outcomes\n",
    "    def state_trans_pd(self): pass # states, actions, outcomes -> post-decision states\n",
    "    def state_trans(self): pass # post-decision states, shocks -> next-period states\n",
    "    def terminal_actions(self): pass # action in last period = zero savings rate = consume everything\n",
    "\n",
    "    # reward\n",
    "    def reward(self): pass # utility\n",
    "    terminal_reward_pd = EconDLSolvers.terminal_reward_pd # defaults to just 0\n",
    "    discount_factor = EconDLSolvers.discount_factor # default to just par.beta\n",
    "\n",
    "    # exploration (not used in DeepSimulate)\n",
    "    draw_exploration_shocks = EconDLSolvers.draw_exploration_shocks # default is normal(0,epsilon_sigma)\n",
    "    exploration = EconDLSolvers.exploration # default is action + eps (clipping is also imposed)\n",
    "\n",
    "    # FOC (only used in DeepFOC)\n",
    "    def eval_equations_FOC(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. <a id='toc3_1_'></a>[Setup and allocate](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(model):\n",
    "    \"\"\" choose parameters \"\"\"\n",
    "\n",
    "    # a. unpack\n",
    "    par = model.par\n",
    "    sim = model.sim\n",
    "\n",
    "    par.seed = 1 # seed for random number generator\n",
    "\n",
    "    # b. model\n",
    "    par.T = 3 # number of periods\n",
    "\n",
    "    # preferences\n",
    "    par.beta = 1.0/1.04 # discount factor\n",
    "    \n",
    "    # income process\n",
    "    par.kappa = 0.2 # income scale\n",
    "    par.sigma_psi = 0.3 # shock, std\n",
    "\n",
    "    # assets\n",
    "    par.r = 0.02 # return rate\n",
    "\n",
    "    # initial states\n",
    "    par.mu_m0 = 1.0 # initial cash-on-hand, mean\n",
    "    par.sigma_m0 = 0.1 # initial cash-on-hand, std\n",
    "\n",
    "    # c. simulation\n",
    "    sim.N = 10_000 # number of agents\n",
    "\n",
    "    # d. misc\n",
    "\n",
    "    # states\n",
    "    par.Nstates = 1 # number of states\n",
    "    par.Nstates_pd = 1 # number of states, post decision\n",
    "\n",
    "    # number of actions and outcomes\n",
    "    par.Nactions = 1 # number of actions\n",
    "    par.Noutcomes = 1 # will be consumption in our case\n",
    "\n",
    "    # number of shocks\n",
    "    par.Nshocks = 1 # psi is the only shock\n",
    "    par.Npsi = 4 # number of quadrature points - not used in DeepSimulate\n",
    "\n",
    "Model.setup = setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate(model):\n",
    "    \"\"\" allocate arrays  \"\"\"\n",
    "\n",
    "    # unpack\n",
    "    par = model.par\n",
    "    sim = model.sim\n",
    "    train = model.train\n",
    "    dtype = train.dtype\n",
    "    device = train.device        \n",
    "\n",
    "    # a. quadrature (not used in DeepSimulate)\n",
    "    par.psi,par.psi_w = log_normal_gauss_hermite(par.sigma_psi,par.Npsi)\n",
    "    par.psi = torch.tensor(par.psi,dtype=dtype,device=device) # convert to tensor\n",
    "    par.psi_w = torch.tensor(par.psi_w,dtype=dtype,device=device) # convert to tensor\n",
    "\n",
    "    # b. simulation (same across models)\n",
    "    sim.states = torch.zeros((par.T,sim.N,par.Nstates),dtype=dtype,device=device) # State-vector\n",
    "    sim.states_pd = torch.zeros((par.T,sim.N,par.Nstates_pd),dtype=dtype,device=device) # post-decision state vector\n",
    "    sim.shocks = torch.zeros((par.T,sim.N,par.Nshocks),dtype=dtype,device=device) # Shock-vector\n",
    "    sim.outcomes = torch.zeros((par.T,sim.N,par.Noutcomes),dtype=dtype,device=device) # outcomes array\n",
    "    sim.actions = torch.zeros((par.T,sim.N,par.Nactions),dtype=dtype,device=device)  # actions array\n",
    "    sim.reward = torch.zeros((par.T,sim.N),dtype=dtype,device=device) # array for utility rewards\n",
    "\n",
    "    sim.R = np.nan # initialize average discounted utility\n",
    "\n",
    "Model.allocate = allocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_train(model):\n",
    "    \"\"\" default parameters for training \"\"\"\n",
    "    \n",
    "    par = model.par\n",
    "    train = model.train\n",
    "    dtype = train.dtype\n",
    "    device = train.device\n",
    "\n",
    "    # a. neural network\n",
    "    train.Nneurons_policy = np.array([50,50]) # number of neurons in hidden layers\n",
    "    train.Nneurons_value = np.array([50,50]) # number of neurons in hidden layers (only used in DeepVPD)\n",
    "    \n",
    "    # b. policy activation functions and clipping\n",
    "    train.policy_activation_final = ['sigmoid'] # action is savings rate in [0,1]\n",
    "    \n",
    "    train.min_actions = torch.tensor([0.0 for _ in range(par.Nactions)],dtype=dtype,device=device) # minimum action value\n",
    "    train.max_actions = torch.tensor([0.9999 for _ in range(par.Nactions)],dtype=dtype,device=device) # maximum action value\t\t\n",
    "    \n",
    "    # c. exploration (not used in DeepSimulate)\n",
    "    train.epsilon_sigma = np.array([0.05])\n",
    "    train.epsilon_sigma_decay = 1.0 # decay rate for epsilon_sigma\n",
    "    train.epsilon_sigma_min = np.array([0.0]) # minimum value for epsilon if decay is used\n",
    "\n",
    "    # d. misc\n",
    "    train.terminal_actions_known = True # not used in DeepSimulate\n",
    "    train.only_time_termination = True\n",
    "    train.K_time = 0.5 # run time in minutes\n",
    "\n",
    "Model.setup_train = setup_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_train(model):\n",
    "    \"\"\" allocate memory training \"\"\"\n",
    "\n",
    "    par = model.par\n",
    "    train = model.train\n",
    "    dtype = train.dtype\n",
    "    device = train.device\n",
    "\n",
    "    # a. training samples (same across models)\n",
    "    train.states = torch.zeros((par.T,train.N,par.Nstates),dtype=dtype,device=device)\n",
    "    train.states_pd = torch.zeros((par.T,train.N,par.Nstates_pd),dtype=dtype,device=device)\n",
    "    train.shocks = torch.zeros((par.T,train.N,par.Nshocks),dtype=dtype,device=device)\n",
    "    train.outcomes = torch.zeros((par.T,train.N,par.Noutcomes),dtype=dtype,device=device)\n",
    "    train.actions = torch.zeros((par.T,train.N,par.Nactions),dtype=dtype,device=device)\n",
    "    train.reward = torch.zeros((par.T,train.N),dtype=dtype,device=device)\n",
    "\n",
    "Model.allocate_train = allocate_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. <a id='toc3_2_'></a>[Drawing shocks and initial states](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_initial_states(model,N,training=False):\n",
    "    \"\"\" draw initial state (m,p,t) \"\"\"\n",
    "\n",
    "    # a. unpack\n",
    "    par = model.par\n",
    "    sigma_m0 = par.sigma_m0\n",
    "\n",
    "    # b. draw cash-on-hand\n",
    "    m0 = par.mu_m0*np.exp(torch.normal(-0.5*sigma_m0**2,sigma_m0,size=(N,)))\n",
    " \n",
    "    # c. store\n",
    "    return torch.stack((m0,),dim=1) # (N,Nstates)\n",
    "\n",
    "def draw_shocks(model,N):\n",
    "    \"\"\" draw shocks \"\"\"\n",
    "\n",
    "    par = model.par\n",
    "\n",
    "    psi_loc = -0.5*par.sigma_psi**2\n",
    "    psi = np.exp(torch.normal(psi_loc,par.sigma_psi,size=(par.T,N,)))\n",
    "\n",
    "    return torch.stack((psi,),dim=-1) # (T,N,Nshocks)\n",
    "\n",
    "Model.draw_initial_states = draw_initial_states\n",
    "Model.draw_shocks = draw_shocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. <a id='toc3_3_'></a>[Quadrature](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(model): # not used in DeepSimulate\n",
    "    \"\"\" quadrature nodes and weights \"\"\"\n",
    "\n",
    "    # a. unpack\n",
    "    par = model.par\n",
    "    psi = par.psi\n",
    "    psi_w = par.psi_w\n",
    "\n",
    "    # b. quadrature nodes and weights\n",
    "    quad = torch.stack((psi,),dim=1) # (Npsi,1)\n",
    "    quad_w = psi_w # (Npsi,)\n",
    "\n",
    "    return quad,quad_w\n",
    "    \n",
    "Model.quad = quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. <a id='toc3_4_'></a>[Outcomes](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes(model,states,actions,t0=0,t=None):\n",
    "\t\"\"\" outcomes \"\"\"\n",
    "\n",
    "\tm = states[...,0]\n",
    "\ta = actions[...,0]\n",
    "\tc = m*(1-a) # intermediary outcome, not a state or action, but still useful\n",
    "\n",
    "\treturn torch.stack((c,),dim=-1) # (T,N,Noutcomes)\n",
    "\n",
    "Model.outcomes = outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. <a id='toc3_5_'></a>[Reward](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(par,c):\n",
    "\t\"\"\" utility \"\"\"\n",
    "\n",
    "\treturn torch.log(c)\n",
    "\n",
    "def reward(model,states,actions,outcomes,t0=0,t=None):\n",
    "\t\"\"\" reward \"\"\"\n",
    "\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\n",
    "\t# b. consumption\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. utility\n",
    "\tu = utility(par,c)\n",
    "\n",
    "\treturn u \n",
    "\n",
    "Model.reward = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. <a id='toc3_6_'></a>[Transition functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{m}_t=m_t-c_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_trans_pd(model,states,actions,outcomes,t0=0,t=None):\n",
    "\t\"\"\" transition to post-decision state \"\"\"\n",
    "\n",
    "\t# Case I: t is None -> t in t0,...,t0+T-1 <= par.T-1:\n",
    "\t#  states.shape = (T,...,Nstates)\n",
    "\t#  actions.shape = (T,...,Nactions)\n",
    "\t#  outcomes.shape = (T,...,Noutcomes)\n",
    "\n",
    "\t# Case II: t in 0,...,T-1, t0 irrelevant:\n",
    "\t#  states.shape = (N,Nstates)\n",
    "\t#  actions.shape = (N,Nactions)\n",
    "\t#  outcomes.shape = (N,Noutcomes)\n",
    "\n",
    "\t# DeepSimulate: never t == None\n",
    "\n",
    "\t# this model: does not depend on t\n",
    "\t\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\n",
    "\t# b. get cash-on-hand and consumption\n",
    "\tm = states[...,0]\n",
    "\tc = outcomes[...,0]\n",
    "\n",
    "\t# c. post-decision\n",
    "\tm_pd = m-c\n",
    "\n",
    "\t# d. finalize\n",
    "\tstates_pd = torch.stack((m_pd,),dim=-1)\n",
    "\treturn states_pd\n",
    "\t# Case I: shape = (T,...,Nstates_pd)\n",
    "\t# Case II: shape = (N,Nstates_pd)\n",
    "\n",
    "Model.state_trans_pd = state_trans_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m_{t+1}=(1+r)\\bar{m}_t + \\kappa\\psi_{t+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_trans(model,states_pd,shocks,t=None):\n",
    "\t\"\"\" state transition with quadrature \"\"\"\n",
    "\n",
    "\t# Case I: t is None -> t in 0,...,T-1 <= par.T-1:\n",
    "\t#  states_pd.shape = (T,N,Nstates_pd)\n",
    "\t#  shocks.shape = (Nquad,Nshocks) [this is quadrature nodes]\n",
    "\t\n",
    "\t# Case II: t in 0,...,T-1, t0 irrelevant:\n",
    "\t#  states_pd.shape = (N,Nstates_pd)\n",
    "\t#  shocks.shape = (N,Nshocks) [this is actual shocks]\n",
    "\n",
    "\t# DeepSimulate: never t == None\n",
    "\n",
    "\t# a. unpack\n",
    "\tpar = model.par\n",
    "\ttrain = model.train\n",
    "\n",
    "\t# b. get post-decision cash-on-hand and shock\n",
    "\tm_pd = states_pd[...,0]\n",
    "\tpsi = shocks[:,0]\n",
    "\n",
    "\tif t is None:\n",
    "\t\tm_pd = EconDLSolvers.expand_to_quad(m_pd,train.Nquad) # (T,N) ==> (T,N,Nquad) Nquad = number of quadrature nodes\n",
    "\t\tpsi = EconDLSolvers.expand_to_states(psi,states_pd) # (Nquad,) ==> (T,N,Nquad)\n",
    "\n",
    "\t# c. future cash-on-hand\n",
    "\tm_plus = (1+par.r)*m_pd + psi*par.kappa\n",
    "\t\n",
    "\t# d. finalize\n",
    "\tstates_plus = torch.stack((m_plus,),dim=-1) # (T,N,1)\t\n",
    "\treturn states_plus\n",
    "\t# Case I: shape = (T,N,Nquad,Nstates)\n",
    "\t# Case II: shape = (N,Nstates)\n",
    "\n",
    "Model.state_trans = state_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. <a id='toc3_7_'></a>[Terminal actions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_actions(model,states):\n",
    "\t\"\"\" terminal actions \"\"\"\n",
    "\n",
    "\t# Case I: states.shape = (1,...,Nstates)\n",
    "\t# Case II: states.shape = (N,Nstates)\n",
    "\t\n",
    "\tpar = model.par\n",
    "\ttrain = model.train\n",
    "\tdtype = train.dtype\n",
    "\tdevice = train.device\n",
    "\t\n",
    "\tactions = torch.zeros((*states.shape[:-1],1),dtype=dtype,device=device)\n",
    "\n",
    "\tif train.algoname == 'DeepFOC':\n",
    "\t\tmultipliers = torch.zeros((*states.shape[:-1],1),dtype=dtype,device=device)\n",
    "\t\tactions = torch.cat((actions,multipliers),dim=-1)\n",
    "\n",
    "\treturn actions \n",
    "\t# Case I: shape = (1,...,Nactions)\n",
    "\t# Case II: shape = (N,Nactions)\t\n",
    "\n",
    "Model.terminal_actions = terminal_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[DeepSimulate](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepSimulate = Model(algoname='DeepSimulate',device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepSimulate.solve(do_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "t_range = range(model_DeepSimulate.par.T)\n",
    "avg_wealth_DeepSimulate = model_DeepSimulate.sim.states[...,0].mean(dim=1).cpu().numpy()\n",
    "avg_consumption_DeepSimulate = model_DeepSimulate.sim.outcomes[...,0].mean(dim=1).cpu().numpy()\n",
    "\n",
    "ax[0].plot(t_range,avg_wealth_DeepSimulate,label='Average wealth')\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('Average wealth')\n",
    "ax[0].set_xticks(t_range)\n",
    "\n",
    "ax[1].plot(t_range,avg_consumption_DeepSimulate,label='Average consumption')\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('Average consumption')\n",
    "ax[1].set_xticks(t_range)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[DeepVPD](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepVPD = Model(algoname='DeepVPD',device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepVPD.solve(do_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "t_range = range(model_DeepVPD.par.T)\n",
    "avg_wealth_DeepVPD = model_DeepVPD.sim.states[...,0].mean(dim=1).cpu().numpy()\n",
    "avg_consumption_DeepVPD = model_DeepVPD.sim.outcomes[...,0].mean(dim=1).cpu().numpy()\n",
    "\n",
    "ax[0].plot(t_range,avg_wealth_DeepSimulate,label='DeepSimulate')\n",
    "ax[0].plot(t_range,avg_wealth_DeepVPD,label='DeepVPD',linestyle='--')\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('Average wealth')\n",
    "ax[0].set_xticks(t_range)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(t_range,avg_consumption_DeepSimulate,label='DeepSimulate')\n",
    "ax[1].plot(t_range,avg_consumption_DeepVPD,label='DeepVPD',linestyle='--')\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('Average consumption')\n",
    "ax[1].set_xticks(t_range)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id='toc6_'></a>[DeepFOC](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KKT conditions for this problem is:\n",
    "$$\n",
    "u'(c_t) = \\beta(1+r)E_t[u'(c_{t+1})] + \\lambda_t \\\\\n",
    "\\lambda_t \\bar{m}_t = \\lambda_t (m_t-c_t) = 0 \\\\\n",
    "\\lambda_t \\geq 0\n",
    "$$\n",
    "where $\\lambda_t$ is a KKT-multiplier on the borrowing constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marg_util_c(c):\n",
    "\t\"\"\" marginal utility of consumption \"\"\"\n",
    "\n",
    "\treturn 1/c\n",
    "\n",
    "def inverse_marg_util(u):\n",
    "\t\"\"\"Inverse function of marginal utility of consumption \"\"\"\n",
    "\n",
    "\treturn 1/u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_equations_FOC(model,states,states_plus,actions,actions_plus,outcomes,outcomes_plus):\n",
    "\t\"\"\" evaluate equations for DeepFOC using KKT conditions \"\"\"\n",
    "\n",
    "\t# states.shape = (T,N,Nstates)\n",
    "\t# states_plus.shape = (T,N,Nquad,Nstates)\n",
    "\t# actions.shape = (T,N,Nactions)\n",
    "\t# actions_plus.shape = (T,N,Nquad,Nactions)\n",
    "\t# outcomes.shape = (T,N,Noutcomes)\n",
    "\t# outcomes_plus.shape = (T,N,Nquad,Noutcomes)\n",
    "\n",
    "\tpar = model.par\n",
    "\ttrain = model.train\n",
    "\n",
    "\t# a. compute consumption and multiplier at time t\n",
    "\tc_t = outcomes[...,0]\n",
    "\tmultiplier_t = actions[...,1] # get mulitplier from actions\n",
    "\n",
    "\t# b. get consumption at time t+1\n",
    "\tc_tplus = outcomes_plus[...,0]\n",
    "\n",
    "\t# c. compute marginal utility at time t+1\n",
    "\tmarg_util_tplus = marg_util_c(c_tplus)\n",
    "\n",
    "\t# d. compute expected marginal utility at time t+1\n",
    "\texp_marg_util_t1 = torch.sum(train.quad_w[None,None,:]*marg_util_tplus,dim=-1)\n",
    "\t\n",
    "\t# e. compute euler equation\n",
    "\tbeta = EconDLSolvers.discount_factor(model,states)\n",
    "\tFOC = inverse_marg_util(beta*(1+par.r)*exp_marg_util_t1+multiplier_t) / c_t - 1\n",
    "\t\n",
    "\t# f. borrowing constraint (slackness condition)\n",
    "\tm_t = states[...,0]\n",
    "\tslackness = multiplier_t*(m_t-c_t) # <= 0\n",
    "\n",
    "\t# g. combine\n",
    "\teq = torch.stack((FOC**2,slackness**2),dim=-1) # (T,N,Nactions)\n",
    "\n",
    "\treturn eq\n",
    "\n",
    "Model.eval_equations_FOC = eval_equations_FOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dict = {'Nactions':2,'KKT':True}\n",
    "train_dict = {\n",
    "    'epsilon_sigma':np.array([0.05,0.0]),\n",
    "    'epsilon_sigma_min':np.array([0.0,0.0]),\n",
    "}\n",
    "model_DeepFOC = Model(algoname='DeepFOC',device='cpu',par=par_dict,train=train_dict)\n",
    "\n",
    "model_DeepFOC.train.eq_w = torch.tensor([1.0,1.0],dtype=torch.float32,device='cpu') # 5 weight on FOC and 1 on budget constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFOC.solve(do_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "t_range = range(model_DeepFOC.par.T)\n",
    "avg_wealth_DeepFOC = model_DeepFOC.sim.states[...,0].mean(dim=1).cpu().numpy()\n",
    "avg_consumption_DeepFOC = model_DeepFOC.sim.outcomes[...,0].mean(dim=1).cpu().numpy()\n",
    "\n",
    "ax[0].plot(t_range,avg_wealth_DeepSimulate,label='DeepSimulate')\n",
    "ax[0].plot(t_range,avg_wealth_DeepVPD,label='DeepVPD', linestyle='--')\n",
    "ax[0].plot(t_range,avg_wealth_DeepFOC,label='DeepFOC', linestyle=':')\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('Average wealth')\n",
    "ax[0].set_xticks(t_range)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(t_range,avg_consumption_DeepSimulate,label='DeepSimulate')\n",
    "ax[1].plot(t_range,avg_consumption_DeepVPD,label='DeepVPD', linestyle='--')\n",
    "ax[1].plot(t_range,avg_consumption_DeepFOC,label='DeepFOC', linestyle=':')\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('Average consumption')\n",
    "ax[1].set_xticks(t_range)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
